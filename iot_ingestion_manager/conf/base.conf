# Copyright 2017 TEAM PER LA TRASFORMAZIONE DIGITALE
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

api.version = "1.0"

kerberos {
  keytab    = "conf/daf.keytab"
  principal = "daf@DAF.GOV.IT"
}

ipa {
  # url   = the url to connect to FreeIPA
  user    = "admin"
  userpwd = ${?FREEIPA_ADMIN_PWD}
}

play {

  crypto.secret = "changeme"

  i18n.langs = ["en"]

  # http.filters = "it.gov.daf.common.filters.FiltersSecurityCORS"

  modules {
    enabled += "play.modules.swagger.SwaggerModule"
    enabled += "it.gov.daf.common.modules.hadoop.HadoopModule"
    enabled += "it.gov.daf.common.modules.authentication.SecurityModule"
  }

  crypto.secret  = ${?PLAY_CRYPTO}

}

swagger.api.basepath = "storage-manager/swagger.json"

pac4j {

  jwt_secret = ${?PAC4J_CRYPTO}

  security {
    rules = [
      {
        "/iot-manager/v1/.*" = {
          authorizers = "_authenticated_"
          clients = "DirectBasicAuthClient, HeaderClient"
        }
      }
    ]
  }
}

spark {

  yarn.jars = "local:/opt/cloudera/parcels/SPARK2/lib/spark2/jars/*"

  serializer = "org.apache.spark.serializer.KryoSerializer"

  speculation = "false"

  io.compression.codec = "lzf"

  shuffle {
    manager         = "sort"
    service.enabled = "true"
  }

  dynamicAllocation {
    enabled          = "true"
    minExecutors     = "4"
    initialExecutors = "4"
  }

  executor {
    cores  = "2"
    memory = "512m"

    extraClassPath   = "/etc/hbase/conf:/opt/cloudera/parcels/CDH/jars/htrace-core-3.2.0-incubating.jar:/opt/cloudera/parcels/CDH/jars/hbase-protocol-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-common-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-server-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-client-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-hadoop-compat-1.2.0-cdh5.12.0.jar"
    extraJavaOptions = "-Djava.security.auth.login.config=/tmp/jaas.conf"
  }

  driver {
    memory = "512m"

    extraClassPath = "/etc/hbase/conf:/opt/cloudera/parcels/CDH/jars/htrace-core-3.2.0-incubating.jar:/opt/cloudera/parcels/CDH/jars/hbase-protocol-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-common-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-server-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-client-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-hadoop-compat-1.2.0-cdh5.12.0.jar"
  }
}

hadoop_conf_dir = "/etc/hadoop/conf"
hbase_conf_dir  = "/etc/hbase/conf"

kafka {
  servers = [
    "localhost:9092" # brokers' host:port
  ]
  group_id      = "group-1" # group id to use when consuming
  num_producers = 1 # sets the number of producer actors to have ready in the pool

  topic { # configuration settings used when creating a topic
    partitions         = 1 # number of partitions
    replication_factor = 1 # amount of replication to use per partition per topic
    cleanup_policy     = delete # indicates how to dispose of rolled log segments whose size or time retention limits have been exceeded [delete, compact]
    compression_type   = uncompressed # indicates what type of compression to use [uncompressed, snappy, lz4, gzip, producer]
    max_message_bytes  = 1048576 # maximum amount of bytes er message, default 1MB
    retention_time     = 7 days # maximum retention for a rolled log before triggering cleanup
  }
}

daf {

  catalog_url = "http://catalog-manager:9000"

  stream {
    validator = "avro" # sets the validator to use when reading transmitted events before pushing onto kafka [avro, none]
  }
}
