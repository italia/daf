# Copyright 2017 TEAM PER LA TRASFORMAZIONE DIGITALE
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Secret key
# ~~~~~
# The secret key is used to secure cryptographics functions.
#
# This must be changed for production, but we recommend not changing it in this file.
#
# See http://www.playframework.com/documentation/latest/ApplicationSecret for more details.
play.crypto.secret = "changeme"

# The application languages
# ~~~~~
play.i18n.langs = [ "en" ]

# Router
# ~~~~~
# Define the Router object to use for this application.
# This router will be looked up first when the application is starting up,
# so make sure this is the entry point.
# Furthermore, it's assumed your route file is named properly.
# So for an application router like `my.application.Router`,
# you may need to define a router file `conf/my.application.routes`.
# Default to Routes in the root package (and conf/routes)
# play.http.router = my.application.Routes

# Database configuration
# ~~~~~
# You can declare as many datasources as you want.
# By convention, the default datasource is named `default`
#
# db.default.driver=org.h2.Driver
# db.default.url="jdbc:h2:mem:play"
# db.default.username=sa
# db.default.password=""

# Evolutions
# ~~~~~
# You can disable evolutions if needed
# play.evolutions.enabled=false

# You can disable evolutions for a specific datasource if necessary
# play.evolutions.db.default.enabled=false

play.modules.enabled += "it.gov.daf.common.modules.hadoop.HadoopModule"
play.modules.enabled += "it.gov.daf.common.modules.authentication.SecurityModule"
play.http.filters     = "it.gov.daf.common.filters.authentication.Filters"

hadoop_conf_dir = "/etc/hadoop/conf"
hbase_conf_dir = "/etc/hbase/conf"
keytab = "/application/conf/daf.keytab"
principal = "daf@PLATFORM.DAF.LOCAL"

pac4j.jwt_secret = "123456789012345678901234567890123"
pac4j.security {
  rules = [
    {
      "/iot-ingestion-manager/v1/.*" = {
        authorizers = "_authenticated_"
        clients = "DirectBasicAuthClient, HeaderClient"
      }
    }
  ]
}
pac4j.authenticator = "test"
pac4j.ldap.url = "ldap://idm.daf.gov.it:389"
pac4j.ldap.user_dn_pattern = "uid=%s,cn=users,cn=accounts,dc=daf,dc=gov,dc=it"
pac4j.ldap.bind_dn =  "uid=admin,cn=users,cn=accounts,dc=daf,dc=gov,dc=it"
pac4j.ldap.bind_pwd = "password"

#Kafka Parameters
bootstrap.servers = "edge1:9092,edge2:9092,edge3:9092"
group.id = "daf-iot-manager"
topic = "daf-iot-events"

#Spark Parameters
batch.duration = 1000
offsets.table.name = "offsets_table"
spark.yarn.jars = "local:/opt/cloudera/parcels/SPARK2/lib/spark2/jars/*"
spark.serializer = "org.apache.spark.serializer.KryoSerializer"
spark.io.compression.codec = "lzf"
spark.speculation = "false"
spark.shuffle.manager= "sort"
spark.shuffle.service.enabled = "true"
spark.dynamicAllocation.enabled = "true"
spark.dynamicAllocation.minExecutors = "4"
spark.dynamicAllocation.initialExecutors = "4"
spark.executor.cores = "2"
spark.executor.memory = "2048m"
spark.driver.extraClassPath = "/etc/hbase/conf:/opt/cloudera/parcels/CDH/jars/htrace-core-3.2.0-incubating.jar:/opt/cloudera/parcels/CDH/jars/hbase-protocol-1.2.0-cdh5.11.1.jar:/opt/cloudera/parcels/CDH/jars/hbase-common-1.2.0-cdh5.11.1.jar:/opt/cloudera/parcels/CDH/jars/hbase-server-1.2.0-cdh5.11.1.jar:/opt/cloudera/parcels/CDH/jars/hbase-client-1.2.0-cdh5.11.1.jar:/opt/cloudera/parcels/CDH/jars/hbase-hadoop-compat-1.2.0-cdh5.11.1.jar"
spark.executor.extraClassPath = "/etc/hbase/conf:/opt/cloudera/parcels/CDH/jars/htrace-core-3.2.0-incubating.jar:/opt/cloudera/parcels/CDH/jars/hbase-protocol-1.2.0-cdh5.11.1.jar:/opt/cloudera/parcels/CDH/jars/hbase-common-1.2.0-cdh5.11.1.jar:/opt/cloudera/parcels/CDH/jars/hbase-server-1.2.0-cdh5.11.1.jar:/opt/cloudera/parcels/CDH/jars/hbase-client-1.2.0-cdh5.11.1.jar:/opt/cloudera/parcels/CDH/jars/hbase-hadoop-compat-1.2.0-cdh5.11.1.jar"
spark.executor.extraJavaOptions = "-Djava.security.auth.login.config=/tmp/jaas.conf"

#OpenTSDB Context Parameters
#opentsdb.context.keytab = "/opt/docker/conf/daf.keytab"
#opentsdb.context.principal = "daf@PLATFORM.DAF.LOCAL"
#opentsdb.context.keytablocaltempdir = "/tmp"
#opentsdb.context.saltwidth = 1
#opentsdb.context.saltbucket = 4