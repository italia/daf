# Dataset

The DAF platform revolves around the concept of *dataset*.
In fact all data stored in the DAF are organized into logical entities called dataset. 

A *dataset* is a combination of metadata, data, and other operational info needed to manage ingestion, update, lookup, creation of views and alike. It is general purpose, in the sense that it can be used to model both batch, streaming, semi-structured data.


## Types of dataset

Dataset in the DAF platform are of three types:

- **Standard datasets** are defined as datasets with national relevance, standardized across data sources (there may be multiple data sources describing the same phenomena, i.e. the bike sharing phenomena can have a data source from "Comune di Milano", another from "Comune di Torino", etc.) and supported by the highest level of information/metadata. They are aimed at describing phenomena that are common nationwide and therefore are based on a detailed set of rules and standardization mechanisms that will make them homogeneous.

- **Ordinary datasets** have "owner" relevance, in the sense that they are defined and generated by a specific owner for its specific usage. They do not obey to a standard nationwide schema, but the owner needs to specify metadata and info about the dataset before ingesting the data into the platform.

- **Raw datasets** are those with the lowest level of information: DAF works more as a storage layer with simple look up mechanism via rest API. They are mostly used to manage Open Data without stringent metadata information.

## Dataset lifecycle

The lifecycle of a dataset is based on three main steps:

1. *Creation of a Dataset*  
An authenticated user registers can add a dataset to the DAF platform filling the [dataset registration form](../gui/registrationForm.md).
Then data related to the registered dataset can be moved from the source towards the DAF. The DAF platform provides APIs both for batch and streaming data ingestion. When data are received by the DAF platform, they are stored as raw data in the DAF datalake. Depending on the information provided by the dataset's owner during the registration phase, a dataset can be converted into standard big data serializazion formats (eg. Avro and Parquet) and 'stored' into operational databases (eg. HBase, Impala, etc). For more information about the ingestion phase see the [Data Ingestion](../data-ingestion) documentation.

2. *Dataset analysis* - once a dataset is available and properly saved in the datalake, the user can move to the analysis phase. The DAF platform provides several tools for the processing and analysing datasets. [Here](???) a list of the available tools.

3. *Data and insights publishing* - ...



