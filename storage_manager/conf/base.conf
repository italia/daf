# Copyright 2017 TEAM PER LA TRASFORMAZIONE DIGITALE
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

api.version = "1.0"

kerberos {
  keytab    = "conf/daf.keytab"
  principal = "daf@DAF.GOV.IT"
}

ipa {
  # url   = the url to connect to FreeIPA
  user    = "admin"
  userpwd = ${?FREEIPA_ADMIN_PWD}
}

play {

  crypto.secret = "changeme"

  i18n.langs = ["en"]

  # http.filters = "it.gov.daf.common.filters.FiltersSecurityCORS"

  modules {
    enabled += "modules.WithSecurityInfoSwaggerModule"
    enabled += "it.gov.daf.common.modules.hadoop.HadoopModule"
    enabled += "it.gov.daf.common.modules.authentication.SecurityModule"
  }

  crypto.secret  = ${?PLAY_CRYPTO}

}

swagger.api.basepath = "storage-manager/swagger.json"

pac4j {

  jwt_secret = ${?PAC4J_CRYPTO}

  security {
    rules = [
      {
        "/storage-manager/v1/.*" = {
          authorizers = "_authenticated_"
          clients = "DirectBasicAuthClient, HeaderClient"
        }
      },
      {
        "/dataset-manager/v1/.*" = {
          authorizers = "_authenticated_"
          clients = "DirectBasicAuthClient, HeaderClient"
        }
      }
    ]
  }
}

#Spark Parameters
batch.duration = 1000

spark {

  yarn.jars = "local:/opt/cloudera/parcels/SPARK2/lib/spark2/jars/*"

  serializer = "org.apache.spark.serializer.KryoSerializer"

  speculation = "false"

  io.compression.codec = "lzf"

  shuffle {
    manager         = "sort"
    service.enabled = "true"
  }

  dynamicAllocation {
    enabled          = "true"
    minExecutors     = "4"
    initialExecutors = "4"
  }

  executor {
    cores  = "2"
    memory = "512m"

    extraClassPath   = "/etc/hbase/conf:/opt/cloudera/parcels/CDH/jars/htrace-core-3.2.0-incubating.jar:/opt/cloudera/parcels/CDH/jars/hbase-protocol-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-common-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-server-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-client-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-hadoop-compat-1.2.0-cdh5.12.0.jar"
    extraJavaOptions = "-Djava.security.auth.login.config=/tmp/jaas.conf"
  }

  driver {
    memory = "512m"

    extraClassPath = "/etc/hbase/conf:/opt/cloudera/parcels/CDH/jars/htrace-core-3.2.0-incubating.jar:/opt/cloudera/parcels/CDH/jars/hbase-protocol-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-common-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-server-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-client-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-hadoop-compat-1.2.0-cdh5.12.0.jar"
  }
}

hadoop_conf_dir = "/etc/hadoop/conf"
hbase_conf_dir  = "/etc/hbase/conf"

impala {
  # host = # an impala daemon host
  # port = # the impala daemon's hive2 server port

  ssl {
    keystore = ${JAVA_HOME}"/jre/lib/security/jssecacerts"  # location of the SSL keystore
    password = ${SSL_KEYSTORE_PWD} # password to the SSL keystore
  }

  memory_estimation_limit = 1024 # the maximum estimated memory in MB that a query is allowed in Impala
}

daf {

  # row_limit = 1000 # optional: maximum number of rows to return in bulk download API when method is quick

  catalog_url = "http://catalog-manager:9000"

  export {

    num_sessions = 1 # the number of concurrent spark sessions that are started in Livy

    size_threshold = 5120 # the maximum file size for direct download in KB

    export_path = "/daf/tmp/export" # the base path in HDFS that will be used for file export jobs

    timeout = 10 minutes

    cleanup {

      poll_interval = 10 minutes # the amount of time the cleanup system will snooze in between attempts

      back_off_interval = 30 seconds # the time the cleanup system will wait before reattempting a failed cleanup

      max_age = 10 minutes # the longest time a file is allowed to exist on HDFS before it is cleaned up

      max_retries = 4 # number of times to reattempt a failed cleanup
    }

    livy {
      host = "localhost:8998"
      # auth = "" # optional: basic auth header information, encoded in base64
      # ssl = true # optional: boolean to indicate whether the http client should use SSL or not (defaults to true)

      client {

        # auth.login.conf = # optional: path to jaas configuration (mandatory with spnego)
        # krb5.conf       = # optional: path to krb5.conf (mandatory with spnego)

        spnego.enabled  = false # optional: enable spnego for http client auth
        auth.scheme = basic # optional: authentication scheme to be used by the client (note that basic will use ldap)

        jars = [
          "lib/"${DAF_STORAGE_MANAGER_ARTIFACT}
        ]

        http {
          connection.timeout = 30 seconds
          connection.socket.timeout = 5 minutes
          connection.idle.timeout = 10 minutes

          content.compress.enable = true

          job.initial_poll_interval = 100 ms
          job.max_poll_interval = 5 seconds

        }
      }
    }
  }
}
