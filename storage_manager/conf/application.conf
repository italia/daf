# Copyright 2017 TEAM PER LA TRASFORMAZIONE DIGITALE
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Secret key
# ~~~~~
# The secret key is used to secure cryptographics functions.
#
# This must be changed for production, but we recommend not changing it in this file.
#
# See http://www.playframework.com/documentation/latest/ApplicationSecret for more details.
play.crypto.secret = "changeme"

# The application languages
# ~~~~~
play.i18n.langs = ["en"]

# Router
# ~~~~~
# Define the Router object to use for this application.
# This router will be looked up first when the application is starting up,
# so make sure this is the entry point.
# Furthermore, it's assumed your route file is named properly.
# So for an application router like `my.application.Router`,
# you may need to define a router file `conf/my.application.routes`.
# Default to Routes in the root package (and conf/routes)
# play.http.router = my.application.Routes

# Database configuration
# ~~~~~
# You can declare as many datasources as you want.
# By convention, the default datasource is named `default`
#
# db.default.driver=org.h2.Driver
# db.default.url="jdbc:h2:mem:play"
# db.default.username=sa
# db.default.password=""

# Evolutions
# ~~~~~
# You can disable evolutions if needed
# play.evolutions.enabled=false

# You can disable evolutions for a specific datasource if necessary
# play.evolutions.db.default.enabled=false

play.modules.enabled += "modules.WithSecurityInfoSwaggerModule"
play.modules.enabled += "it.gov.daf.common.modules.hadoop.HadoopModule"
play.modules.enabled += "it.gov.daf.common.modules.authentication.SecurityModule"
#play.http.filters     = "it.gov.daf.common.filters.FiltersSecurityCORS"

swagger.api.basepath = "storage-manager/swagger.json"

api.version = "1.0"
max_number_of_rows = 1000
chunk_size = 100
spark_driver_memory = 512M

hadoop_conf_dir = "/etc/hadoop/conf"
hbase_conf_dir = "/etc/hbase/conf"
keytab = "conf/daf.keytab"
principal = "daf@DAF.GOV.IT"

pac4j.jwt_secret = "123456789012345678901234567890123"
pac4j.security {
  rules = [
    {
      "/storage-manager/v1/.*" = {
        authorizers = "_authenticated_"
        clients = "DirectBasicAuthClient, HeaderClient"
      }
    },
    {
      "/dataset-manager/v1/.*" = {
        authorizers = "_authenticated_"
        clients = "DirectBasicAuthClient, HeaderClient"
      }
    }
  ]
}
#pac4j.authenticator = "test"
#pac4j.ldap.url = "ldap://idm.daf.gov.it:389"
#pac4j.ldap.user_dn_pattern = "uid=%s,cn=users,cn=accounts,dc=daf,dc=gov,dc=it"
#pac4j.ldap.bind_dn =  "uid=admin,cn=users,cn=accounts,dc=daf,dc=gov,dc=it"
#pac4j.ldap.bind_pwd = "****"

kudu.master = "master:7051"

#Spark Parameters
batch.duration = 1000
spark.yarn.jars = "local:/opt/cloudera/parcels/SPARK2/lib/spark2/jars/*"
spark.serializer = "org.apache.spark.serializer.KryoSerializer"
spark.io.compression.codec = "lzf"
spark.speculation = "false"
spark.shuffle.manager= "sort"
spark.shuffle.service.enabled = "true"
spark.dynamicAllocation.enabled = "true"
spark.dynamicAllocation.minExecutors = "4"
spark.dynamicAllocation.initialExecutors = "4"
spark.executor.cores = "2"
spark.executor.memory = "2048m"
spark.streaming.backpressure.enabled = "true"
spark.streaming.kafka.maxRatePerPartition = "500"
spark.driver.extraClassPath = "/etc/hbase/conf:/opt/cloudera/parcels/CDH/jars/htrace-core-3.2.0-incubating.jar:/opt/cloudera/parcels/CDH/jars/hbase-protocol-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-common-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-server-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-client-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-hadoop-compat-1.2.0-cdh5.12.0.jar"
spark.executor.extraClassPath = "/etc/hbase/conf:/opt/cloudera/parcels/CDH/jars/htrace-core-3.2.0-incubating.jar:/opt/cloudera/parcels/CDH/jars/hbase-protocol-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-common-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-server-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-client-1.2.0-cdh5.12.0.jar:/opt/cloudera/parcels/CDH/jars/hbase-hadoop-compat-1.2.0-cdh5.12.0.jar"
spark.executor.extraJavaOptions = "-Djava.security.auth.login.config=/tmp/jaas.conf"

daf.limit_row = 1000

daf {
  catalog-url = "http://catalog-manager.default.svc.cluster.local:9000"

  export {

    num_sessions = 1 # the number of concurrent spark sessions that are started in Livy

    size_threshold = 5120 # the maximum file size for direct download in KB

    export_path = "/daf/tmp/export" # the base path in HDFS that will be used for file export jobs

    timeout = 10 minutes

    cleanup {

      poll_interval = 5 minutes # the amount of time the cleanup system will snooze in between attempts

      back_off_interval = 30 seconds # the time the cleanup system will wait before reattempting a failed cleanup

      max_age = 5 minutes # the longest time a file is allowed to exist on HDFS before it is cleaned up

      max_retries = 4 # number of times to reattempt a failed cleanup
    }

    livy {
      host = "localhost:8998"
      # auth = "" # optional: basic auth header information, encoded in base64

      client {

        # spnego.enabled            = false
        # auth.login.conf           = # optional: path to jaas configuration (mandatory with spnego)
        # krb5.conf                 = # optional: path to krb5.conf (mandatory with spnego)

        auth.scheme = basic # optional: authentication scheme to be used by the client (note that basic will use ldap)

        http {
          connection.timeout = 10 seconds
          connection.socket.timeout = 5 minutes
          connection.idle.timeout = 10 minutes

          content.compress.enable = true

          job.initial_poll_interval = 100 ms
          job.max_poll_interval = 5 seconds

        }
      }
    }
  }
}
